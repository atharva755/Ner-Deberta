{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab68a5c-4053-4982-ade1-b1e920d7afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import gradio as gr\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7dd6cd-6f3b-4e7c-bada-9a71f29987df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load trained NER model\n",
    "model_path = \"./ner_deberta_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1dc75e-7d12-45e0-9943-5c74813f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df80d897-0428-47b2-b075-88180488bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    max_chunk_tokens = 1024\n",
    "    min_words_required = 20\n",
    "\n",
    "    if len(text.strip().split()) < min_words_required:\n",
    "        return \"‚ö†Ô∏è Text too short for meaningful summary.\"\n",
    "\n",
    "    try:\n",
    "        paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "        chunks, current_chunk = [], \"\"\n",
    "\n",
    "        for para in paragraphs:\n",
    "            if len((current_chunk + para).split()) <= max_chunk_tokens:\n",
    "                current_chunk += \" \" + para\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        summary_chunks = []\n",
    "        for chunk in chunks:\n",
    "            output = summarizer(chunk, max_length=130, min_length=30, do_sample=False)\n",
    "            if output and \"summary_text\" in output[0]:\n",
    "                summary_chunks.append(output[0][\"summary_text\"])\n",
    "\n",
    "        return \" \".join(summary_chunks) if summary_chunks else \"‚ö†Ô∏è Summary could not be generated.\"\n",
    "\n",
    "    except Exception:\n",
    "        return \"‚ùå Could not generate summary due to error or model limit.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92aa6b96-d33a-4e0d-9e18-eb01b1c573cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(text, confidence_threshold):\n",
    "    entity_groups = defaultdict(dict)\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            entities = ner_pipeline(line)\n",
    "            for ent in entities:\n",
    "                word = ent[\"word\"].strip()\n",
    "                group = ent[\"entity_group\"]\n",
    "                score = round(ent[\"score\"], 2)\n",
    "\n",
    "                if score >= confidence_threshold:\n",
    "                    if word not in entity_groups[group] or score > entity_groups[group][word]:\n",
    "                        entity_groups[group][word] = score\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    results = []\n",
    "    for group, words in entity_groups.items():\n",
    "        for word, score in sorted(words.items()):\n",
    "            results.append([group, word, f\"{score:.2f}\"])\n",
    "\n",
    "    summary_text = generate_summary(text)\n",
    "    return results, summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91a0426-e06b-4579-af3d-a165a79fdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"NER + Summarizer UI\") as demo:\n",
    "    gr.Markdown(\"## üß† Named Entity Recognition (NER) + AI Summary\")\n",
    "    gr.Markdown(\"Upload a `.txt` file or paste your text. Adjust confidence threshold to control what gets shown.\")\n",
    "\n",
    "    with gr.Tab(\"üì§ Upload File\"):\n",
    "        file_input = gr.File(file_types=[\".txt\"], label=\"Upload Text File\")\n",
    "        file_confidence = gr.Slider(minimum=0.5, maximum=1.0, value=0.85, step=0.01, label=\"Confidence Threshold\")\n",
    "        file_output = gr.Dataframe(headers=[\"Entity Type\", \"Text\", \"Confidence\"], label=\"NER Output\")\n",
    "        file_summary = gr.Textbox(label=\"üìù AI-Generated Summary\", lines=4, interactive=False)\n",
    "\n",
    "    with gr.Tab(\"‚úçÔ∏è Paste Text\"):\n",
    "        text_input = gr.Textbox(label=\"Paste Your Text Here\", lines=10, placeholder=\"Enter paragraph or sentences...\")\n",
    "        text_confidence = gr.Slider(minimum=0.5, maximum=1.0, value=0.85, step=0.01, label=\"Confidence Threshold\")\n",
    "        text_output = gr.Dataframe(headers=[\"Entity Type\", \"Text\", \"Confidence\"], label=\"NER Output\")\n",
    "        text_summary = gr.Textbox(label=\"üìù AI-Generated Summary\", lines=4, interactive=False)\n",
    "\n",
    "    file_input.change(\n",
    "        lambda file, threshold: run_pipeline(read_text_file(file.name), threshold) if file else ([], \"\"),\n",
    "        inputs=[file_input, file_confidence],\n",
    "        outputs=[file_output, file_summary],\n",
    "    )\n",
    "\n",
    "    text_input.change(\n",
    "        run_pipeline,\n",
    "        inputs=[text_input, text_confidence],\n",
    "        outputs=[text_output, text_summary],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e51810b-e187-4382-b78d-0e25de247981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfbf82-9056-4dfd-b956-c5bd47215f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
