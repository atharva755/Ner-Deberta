{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73256205-66c5-4c96-9ed4-a83285674a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de4fc6b-2bd3-4d27-9971-e744dac73a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 1: Path setup ======\n",
    "DATA_PATH = r\"C:\\Users\\atharva\\CDAC\\Project Ner\\ner_crime_training_300_sentences.txt\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "OUTPUT_DIR = \"./ner_deberta_model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7c0b2a-70c7-4527-ba6d-6483c5017b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\New folder\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====== Step 2: Load tokenizer ======\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33fe7bb-476b-45bf-92e6-6aa00a6c4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 3: Read and structure CoNLL data ======\n",
    "def read_conll(filepath):\n",
    "    tokens, labels, all_data = [], [], []\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens and labels and len(tokens) == len(labels):\n",
    "                    all_data.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "                tokens, labels = [], []\n",
    "                continue\n",
    "            splits = line.split()\n",
    "            if len(splits) >= 2:\n",
    "                tokens.append(splits[0])\n",
    "                labels.append(splits[-1])\n",
    "    if tokens and labels and len(tokens) == len(labels):\n",
    "        all_data.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return all_data\n",
    "\n",
    "all_data = read_conll(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48b40ed-e217-4f38-9ac8-76452b0fe057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 4: Label encoding (FIXED) ======\n",
    "unique_labels = sorted(set(label for row in all_data for label in row[\"ner_tags\"]))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# ✅ Convert all ner_tags to integer IDs\n",
    "for row in all_data:\n",
    "    encoded_tags = []\n",
    "    for label in row[\"ner_tags\"]:\n",
    "        if label in label_to_id:\n",
    "            encoded_tags.append(label_to_id[label])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label found: {label}\")\n",
    "    row[\"ner_tags\"] = encoded_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616a04cf-0047-4ee9-909f-e54d3f623a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 5: Convert to HuggingFace Dataset ======\n",
    "dataset = Dataset.from_list(all_data)\n",
    "if len(all_data) >= 10:\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    datasets = DatasetDict(train=dataset[\"train\"], test=dataset[\"test\"])\n",
    "else:\n",
    "    datasets = DatasetDict(train=dataset, test=dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49abc5d3-fc70-4e03-a359-72bf4afac4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca78d567c78481d8069e93cbfca73c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216678f17b0848beb5fd4e3a574530fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====== Step 6: Tokenize and align labels ======\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Add max_length to suppress truncation warning\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                try:\n",
    "                    label_ids.append(examples[\"ner_tags\"][i][word_idx])\n",
    "                except Exception as e:\n",
    "                    print(\"⚠️ Error aligning labels:\", e)\n",
    "                    print(\"Tokens:\", examples[\"tokens\"][i])\n",
    "                    print(\"NER tags:\", examples[\"ner_tags\"][i])\n",
    "                    raise\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 🧪 Safe check before mapping\n",
    "for row in all_data:\n",
    "    for label in row[\"ner_tags\"]:\n",
    "        assert isinstance(label, int), f\"❌ Found non-integer label: {label}\"\n",
    "\n",
    "# 🔁 Apply to dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb5e231-729b-4890-82df-7d27cb66d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ====== Step 7: Load model ======\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546715c0-8946-4658-890b-59363028cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 8: Evaluation metrics ======\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        temp_pred, temp_label = [], []\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:\n",
    "                temp_pred.append(id_to_label[p_i])\n",
    "                temp_label.append(id_to_label[l_i])\n",
    "        true_predictions.append(temp_pred)\n",
    "        true_labels.append(temp_label)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4968a3b-0796-414e-95c4-f182b9d988e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 9: Training arguments ======\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193da4c4-1408-4b76-800b-d816225c07ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New folder\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 04:31, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.081798</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved to: ./ner_deberta_model\n"
     ]
    }
   ],
   "source": [
    "# ====== Step 10: Trainer setup ======\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ====== Step 11: Train and save ======\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"✅ Training complete. Model saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a8b1-0259-485f-b61d-abac3f25a084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
